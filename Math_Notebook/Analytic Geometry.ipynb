{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82e4195-272d-48bb-9b8c-10f42e36418a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Vectors as Geometric Objects in Analytic Geometry**\n",
    "\n",
    "Understanding vectors in linear algebra goes beyond their coordinate representation. Vectors can be seen as arrows in space, having both magnitude (length) and direction. Here are some key ideas and examples:\n",
    "\n",
    "1. **Vectors as Arrows:**  \n",
    "   Think of a vector as an arrow with a certain length and direction. For example, the vector $(3,4)$ in $\\mathbb{R}^2$ is visualized as an arrow starting at the origin and ending at the point $(3,4)$. Its length is calculated as $\\sqrt{3^2+4^2} = 5$.\n",
    "\n",
    "2. **Free Vectors vs. Bound Vectors:**  \n",
    "   Vectors are considered \"free,\" meaning that the vector $(3,4)$ represents the same displacement regardless of where it is placed in space. Only its magnitude and direction matter, not its exact position.\n",
    "\n",
    "3. **Operations on Vectors:**  \n",
    "   - **Addition:** You add vectors by placing the tail of one arrow at the head of another. For instance, adding $(3,4)$ and $(1,2)$ results in $(4,6)$, which is the diagonal of the parallelogram formed by the two arrows.  \n",
    "   - **Scalar Multiplication:** Multiplying a vector by a scalar changes its magnitude but not its direction (unless the scalar is negative, which reverses the direction). For example, $2\\cdot(3,4)$ gives $(6,8)$.\n",
    "\n",
    "4. **Geometric Interpretations in Analytic Geometry:**  \n",
    "   - A **line** can be represented as all points of the form $P = P_0 + t\\,v$, where $P_0$ is a fixed point and $v$ is a direction vector.  \n",
    "   - A **plane** in $\\mathbb{R}^3$ can be described as $P = P_0 + s\\,v + t\\,w$, where $v$ and $w$ are independent direction vectors.\n",
    "\n",
    "5. **Changing Basis and Invariant Properties:**  \n",
    "   When you change the basis of a vector space, the coordinates of a vector change but its intrinsic properties, such as magnitude and direction, do not. For example, the vector $(3,4)$ always has a magnitude of 5 regardless of the basis.\n",
    "\n",
    "6. **Example – Rotation:**  \n",
    "   If you rotate the vector $(3,4)$ by $90^\\circ$ counterclockwise, its new coordinates become $(-4,3)$ while the magnitude remains $5$. This shows that even though the coordinate representation changes, the length and the geometric direction (relative to the new orientation) are preserved.\n",
    "\n",
    "**Summary:**  \n",
    "- Vectors are visualized as arrows with a specific length and direction.\n",
    "- Their geometric properties (magnitude, direction, and relationships like parallelism) remain invariant under a change of basis, even though their coordinates may change.\n",
    "- Analytic geometry uses these ideas to solve problems by translating geometric operations (like rotation, translation, and scaling) into algebraic operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d55e11-7ad1-4999-b144-c4c014d1036c",
   "metadata": {},
   "source": [
    "# Norms\n",
    "\n",
    "A norm on a vector space is a function that assigns a non-negative real number to each vector, representing its \"length\" or \"magnitude.\" Norms capture the idea of distance and size in the space.\n",
    "\n",
    "**Properties of Norms:**  \n",
    "A function $\\|\\cdot\\|: V \\to \\mathbb{R}$ is a norm if, for all vectors $x,y\\in V$ and any scalar $\\alpha$, it satisfies:\n",
    "1. **Positivity:** $\\|x\\|\\ge 0$, and $\\|x\\|=0$ if and only if $x=0$.\n",
    "2. **Homogeneity:** $\\|\\alpha x\\| = |\\alpha|\\,\\|x\\|$.\n",
    "3. **Triangle Inequality:** $\\|x+y\\| \\le \\|x\\|+\\|y\\|$.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples of Norms:**\n",
    "\n",
    "1. **Euclidean Norm (2-norm):**  \n",
    "   For $x=(x_1,x_2,\\dots,x_n)\\in\\mathbb{R}^n$,  \n",
    "   $$\n",
    "   \\|x\\|_2 = \\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}.\n",
    "   $$\n",
    "   *Example:* For $x=(3,4)$, $\\|x\\|_2 = \\sqrt{3^2+4^2} = 5$.\n",
    "\n",
    "2. **1-Norm (Manhattan Norm):**  \n",
    "   $$\n",
    "   \\|x\\|_1 = |x_1|+|x_2|+\\cdots+|x_n|.\n",
    "   $$\n",
    "   *Example:* For $x=(3,-4)$, $\\|x\\|_1 = |3|+|-4| = 3+4 = 7$.\n",
    "\n",
    "3. **Infinity Norm (Max Norm):**  \n",
    "   $$\n",
    "   \\|x\\|_\\infty = \\max\\{|x_1|, |x_2|, \\dots, |x_n|\\}.\n",
    "   $$\n",
    "   *Example:* For $x=(3,-4,2)$, $\\|x\\|_\\infty = 4$.\n",
    "\n",
    "4. **Frobenius Norm (for matrices):**  \n",
    "   For a matrix $A=[a_{ij}]$,  \n",
    "   $$\n",
    "   \\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises:**\n",
    "\n",
    "1. **Exercise 1:**  \n",
    "   Compute the 2-norm and 1-norm for the vector $x=(1,-2,2)$.\n",
    "\n",
    "2. **Exercise 2:**  \n",
    "   Prove that for any vector $x\\in\\mathbb{R}^n$,  \n",
    "   $$\n",
    "   \\|x\\|_\\infty \\le \\|x\\|_2 \\le \\sqrt{n}\\,\\|x\\|_\\infty.\n",
    "   $$\n",
    "\n",
    "3. **Exercise 3:**  \n",
    "   Let $x=(3,4)$ in $\\mathbb{R}^2$. Verify the triangle inequality for the 2-norm by checking that  \n",
    "   $$\n",
    "   \\|(3,4)+(1,-1)\\|_2 \\le \\|(3,4)\\|_2 + \\|(1,-1)\\|_2.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Norms provide a measure of vector \"size\" and are essential in various applications, including error analysis, optimization, and machine learning. They help us quantify distances and ensure that different methods for measuring vector length are consistent with geometric intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d1e8d-a8b3-4047-9a5d-f9fea77dc71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78116b-60e2-4e02-9819-f048e70bf7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "836a0fd1-5d2d-4b06-aa63-e53aefa031b1",
   "metadata": {},
   "source": [
    "***Problem : Implement a Python function to compute the Lp norm of a vector.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbcdaf5-58bf-4949-81e2-c6d045d97a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm: 12.0\n",
      "L2 norm: 7.0710678118654755\n",
      "L∞ norm: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lp_norm(x, p):\n",
    "    \"\"\"\n",
    "    Compute the Lp norm of a vector x.\n",
    "\n",
    "    Parameters:\n",
    "    x (list or numpy array): The input vector.\n",
    "    p (int or float): The order of the norm.\n",
    "\n",
    "    Returns:\n",
    "    float: The Lp norm of x.\n",
    "    \"\"\"\n",
    "    x = np.array(x)  # Convert input to numpy array\n",
    "    if p == np.inf:\n",
    "        return np.max(np.abs(x))  # L∞ norm\n",
    "    else:\n",
    "        return np.sum(np.abs(x) ** p) ** (1 / p)  # Lp norm\n",
    "\n",
    "# Example usage\n",
    "x = [3, -4, 5]\n",
    "print(\"L1 norm:\", lp_norm(x, 1))          # L1 norm\n",
    "print(\"L2 norm:\", lp_norm(x, 2))          # L2 norm\n",
    "print(\"L∞ norm:\", lp_norm(x, np.inf))     # L∞ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5e316-194d-4f19-8329-c11072afed45",
   "metadata": {},
   "source": [
    "# Inner Products\n",
    "\n",
    "\n",
    "The **inner product** of two vectors $ \\mathbf{u} $ and $ \\mathbf{v} $ in an $ n $-dimensional real space is defined as:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\sum_{i=1}^{n} u_i v_i\n",
    "$$\n",
    "\n",
    "For example, if:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} =\n",
    "\\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad\n",
    "\\mathbf{v} =\n",
    "\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, the inner product is:\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{u}, \\mathbf{v} \\rangle = (2 \\times 1) + (3 \\times -1) + (4 \\times 2) = 2 - 3 + 8 = 7\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Inner Product Properties**\n",
    "1. **Commutativity:** $ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{v}, \\mathbf{u} \\rangle $\n",
    "2. **Linearity:** $ \\langle a\\mathbf{u} + b\\mathbf{v}, \\mathbf{w} \\rangle = a\\langle \\mathbf{u}, \\mathbf{w} \\rangle + b\\langle \\mathbf{v}, \\mathbf{w} \\rangle $\n",
    "3. **Positivity:** $ \\langle \\mathbf{u}, \\mathbf{u} \\rangle \\geq 0 $ and $ \\langle \\mathbf{u}, \\mathbf{u} \\rangle = 0 $ if and only if $ \\mathbf{u} = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8da61-817f-4836-9283-a4e13e88f031",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "The **dot product** of two vectors $ \\mathbf{u} $ and $ \\mathbf{v} $ in $ \\mathbb{R}^n $ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i\n",
    "$$\n",
    "\n",
    "For example, if:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} =\n",
    "\\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad\n",
    "\\mathbf{v} =\n",
    "\\begin{bmatrix} 4 \\\\ -1 \\\\ 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, the dot product is:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = (3 \\times 4) + (2 \\times -1) + (1 \\times 2) = 12 - 2 + 2 = 12\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of the Dot Product:**\n",
    "1. **Commutative:** $ \\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u} $\n",
    "2. **Distributive:** $ \\mathbf{u} \\cdot (\\mathbf{v} + \\mathbf{w}) = \\mathbf{u} \\cdot \\mathbf{v} + \\mathbf{u} \\cdot \\mathbf{w} $\n",
    "3. **Scalar Multiplication:** $ (c \\mathbf{u}) \\cdot \\mathbf{v} = c (\\mathbf{u} \\cdot \\mathbf{v}) $\n",
    "4. **Zero Vector:** If $ \\mathbf{u} \\cdot \\mathbf{v} = 0 $, then $ \\mathbf{u} $ and $ \\mathbf{v} $ are **orthogonal** (perpendicular).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e842a3d-c244-4024-ac4e-64f5e8e8c86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([3, 2, 1])\n",
    "b = np.array([4, -1, 2])\n",
    "\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31673022-e86c-4a7c-8df2-48a371588080",
   "metadata": {},
   "source": [
    "## Genaral Inner Products\n",
    "\n",
    "\n",
    "An **inner product** is a function that takes two vectors and produces a scalar, satisfying certain properties:\n",
    "\n",
    "1. **Conjugate Symmetry**:  \n",
    "   $$ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle} $$  \n",
    "   (For real-valued inner products, this reduces to **symmetry**)\n",
    "\n",
    "2. **Linearity in the First Argument**:  \n",
    "   $$ \\langle c\\mathbf{u} + \\mathbf{v}, \\mathbf{w} \\rangle = c\\langle \\mathbf{u}, \\mathbf{w} \\rangle + \\langle \\mathbf{v}, \\mathbf{w} \\rangle $$  \n",
    "\n",
    "3. **Positive-Definiteness**:  \n",
    "   $$ \\langle \\mathbf{v}, \\mathbf{v} \\rangle \\geq 0 $$  \n",
    "   and  \n",
    "   $$ \\langle \\mathbf{v}, \\mathbf{v} \\rangle = 0 \\text{ if and only if } \\mathbf{v} = 0 $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Examples of Inner Products**  \n",
    "\n",
    "1. **Standard Dot Product in $ \\mathbb{R}^n $**  \n",
    "   $$ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\sum_{i=1}^{n} u_i v_i $$  \n",
    "   This is the standard Euclidean inner product.\n",
    "\n",
    "2. **Complex Inner Product in $ \\mathbb{C}^n $**  \n",
    "   $$ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\sum_{i=1}^{n} u_i \\overline{v_i} $$  \n",
    "   This includes **complex conjugation** for vectors in complex space.\n",
    "\n",
    "3. **Integral Inner Product (Functional Analysis)**  \n",
    "   $$ \\langle f, g \\rangle = \\int_a^b f(x) g(x) \\,dx $$  \n",
    "   Used in function spaces (e.g., Fourier series, Hilbert spaces).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33640631-8458-43b4-a0e0-b0253b829e73",
   "metadata": {},
   "source": [
    "## Symmetric, Positive Definite Matrices\n",
    "\n",
    "\n",
    "A **symmetric matrix** is a square matrix that is equal to its transpose:  \n",
    "$$ A = A^T $$  \n",
    "\n",
    "A **positive definite matrix** is a symmetric matrix where all its eigenvalues are positive, meaning:  \n",
    "$$ \\mathbf{x}^T A \\mathbf{x} > 0 \\quad \\text{for all nonzero } \\mathbf{x} \\in \\mathbb{R}^n $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Properties of Symmetric Matrices**  \n",
    "1. **Real Eigenvalues**: All eigenvalues of a symmetric matrix are real.  \n",
    "2. **Orthogonal Eigenvectors**: Eigenvectors corresponding to distinct eigenvalues are orthogonal.  \n",
    "3. **Spectral Theorem**: Any symmetric matrix can be diagonalized by an orthogonal matrix $ Q $:  \n",
    "   $$ A = Q \\Lambda Q^T $$  \n",
    "   where $ \\Lambda $ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of Positive Definite Matrices**  \n",
    "1. **All Principal Minors are Positive**: Leading principal minors (determinants of upper-left $ k \\times k $ submatrices) must be positive.  \n",
    "2. **Cholesky Decomposition**: Any positive definite matrix can be decomposed as:  \n",
    "   $$ A = LL^T $$  \n",
    "   where $ L $ is a lower triangular matrix.  \n",
    "3. **Positive Eigenvalues**: All eigenvalues of a positive definite matrix are strictly greater than zero.  \n",
    "\n",
    "---\n",
    "\n",
    "**Example of a Symmetric, Positive Definite Matrix**  \n",
    "Consider:  \n",
    "$$ A = \\begin{bmatrix} 4 & 2 \\\\ 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "- It is symmetric since $ A^T = A $.  \n",
    "- Compute eigenvalues by solving:  \n",
    "  $$ \\det(A - \\lambda I) = 0 $$  \n",
    "  $$ \\begin{vmatrix} 4-\\lambda & 2 \\\\ 2 & 3-\\lambda \\end{vmatrix} = 0 $$  \n",
    "  Expanding:  \n",
    "  $ (4 - \\lambda)(3 - \\lambda) - (2)(2) = 0 $\n",
    "  $ 12 - 4\\lambda - 3\\lambda + \\lambda^2 - 4 = 0 $  \n",
    "  $ \\lambda^2 - 7\\lambda + 8 = 0 $  \n",
    "  $ (\\lambda - 4)(\\lambda - 3) = 0 $  \n",
    "  The eigenvalues are $ \\lambda_1 = 4, \\lambda_2 = 3 $, which are both positive, confirming that $ A $ is positive definite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367ab3e-a101-428a-8124-d4d3a601a1f2",
   "metadata": {},
   "source": [
    "# Lengths and Distances\n",
    " \n",
    "\n",
    "The **length (or norm)** of a vector measures its magnitude. The **distance** between two vectors quantifies how far apart they are in a vector space.\n",
    "\n",
    "---\n",
    "\n",
    "**Vector Norm (Length of a Vector)**  \n",
    "The **norm** (or length) of a vector $ \\mathbf{v} \\in \\mathbb{R}^n $ is given by:  \n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2} $$  \n",
    "\n",
    "For a vector $ \\mathbf{v} = (v_1, v_2, ..., v_n) $, the norm is:  \n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} $$  \n",
    "\n",
    "**Example:**  \n",
    "For $ \\mathbf{v} = (3, 4) $ in $ \\mathbb{R}^2 $:  \n",
    "$$ \\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Distance Between Two Vectors**  \n",
    "The **Euclidean distance** between two vectors $ \\mathbf{u} $ and $ \\mathbf{v} $ in $ \\mathbb{R}^n $ is given by:  \n",
    "$$ d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2} $$  \n",
    "\n",
    "**Example:**  \n",
    "Let $ \\mathbf{u} = (1,2) $ and $ \\mathbf{v} = (4,6) $ in $ \\mathbb{R}^2 $. The distance is:  \n",
    "$$ d(\\mathbf{u}, \\mathbf{v}) = \\sqrt{(4-1)^2 + (6-2)^2} = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5 $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Norm Properties:**  \n",
    "1. **Non-negativity:** $ \\|\\mathbf{v}\\| \\geq 0 $, and $ \\|\\mathbf{v}\\| = 0 $ if and only if $ \\mathbf{v} = \\mathbf{0} $.  \n",
    "2. **Scaling:** $ \\|\\alpha \\mathbf{v}\\| = |\\alpha| \\|\\mathbf{v}\\| $ for any scalar $ \\alpha $.  \n",
    "3. **Triangle Inequality:** $ \\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\| $.  \n",
    "4. **Pythagorean Theorem:** If $ \\mathbf{u} $ and $ \\mathbf{v} $ are orthogonal, then:  \n",
    "   $$ \\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2 $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Other Norms (Different Ways to Measure Length):**  \n",
    "1. **$ p $-Norm (Generalized Norm):**  \n",
    "   $$ \\|\\mathbf{v}\\|_p = \\left( \\sum_{i=1}^{n} |v_i|^p \\right)^{\\frac{1}{p}} $$  \n",
    "   - **Manhattan Norm ($ p = 1 $):** $ \\|\\mathbf{v}\\|_1 = \\sum_{i=1}^{n} |v_i| $  \n",
    "   - **Euclidean Norm ($ p = 2 $):** $ \\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2} $  \n",
    "   - **Infinity Norm ($ p \\to \\infty $):** $ \\|\\mathbf{v}\\|_{\\infty} = \\max |v_i| $  \n",
    "\n",
    "**Example:**  \n",
    "For $ \\mathbf{v} = (3, -4) $:  \n",
    "- **Manhattan norm:** $ \\|\\mathbf{v}\\|_1 = |3| + |-4| = 7 $  \n",
    "- **Euclidean norm:** $ \\|\\mathbf{v}\\|_2 = \\sqrt{3^2 + (-4)^2} = 5 $  \n",
    "- **Infinity norm:** $ \\|\\mathbf{v}\\|_{\\infty} = \\max(|3|, |-4|) = 4 $  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec180661-61dd-443e-a2e4-6548c155b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean norm of vector v is: 5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.array([3, 4])\n",
    "\n",
    "n = np.linalg.norm(v)\n",
    "print(f'The Euclidean norm of vector v is: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750399a7-f336-413c-9c6b-9e7f7525cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between u and v is: 5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.array([1, 2])\n",
    "u = np.array([4, 6])\n",
    "\n",
    "de = np.linalg.norm(u-v)\n",
    "print(f'The Euclidean distance between u and v is: {de}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85667542-aaca-47dd-89f1-20ecee95cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Manhatten distance between u and v is: 7\n"
     ]
    }
   ],
   "source": [
    "# Manhattan distance\n",
    "import numpy as np\n",
    "v = np.array([1, 2])\n",
    "u = np.array([4, 6])\n",
    "\n",
    "dm = np.sum(np.abs(u-v))\n",
    "print(f'The Manhatten distance between u and v is: {dm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64c5a0-899e-4bfb-b766-df9b9b2c9f14",
   "metadata": {},
   "source": [
    "# Angles and Orthogonality\n",
    "\n",
    "\n",
    "Understanding angles and orthogonality is crucial in linear algebra, geometry, and machine learning. The angle between vectors helps determine their directional similarity, while orthogonality plays a key role in projections, feature engineering, and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**Angle Between Two Vectors**    \n",
    "\n",
    "The angle $ \\theta $ between two nonzero vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n $ is given by the **cosine similarity formula**:  \n",
    "\n",
    "$$ \\cos \\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} $$  \n",
    "\n",
    "where:\n",
    "- $ \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i $ (dot product)\n",
    "- $ \\|\\mathbf{u}\\| $ and $ \\|\\mathbf{v}\\| $ are the Euclidean norms (magnitudes) of the vectors.\n",
    "\n",
    "**Example:**  \n",
    "Let $ \\mathbf{u} = (1, 2) $ and $ \\mathbf{v} = (3, 4) $, then:  \n",
    "\n",
    "1. Compute the dot product:  \n",
    "   $$ \\mathbf{u} \\cdot \\mathbf{v} = (1)(3) + (2)(4) = 3 + 8 = 11 $$  \n",
    "\n",
    "2. Compute the magnitudes:  \n",
    "   $$ \\|\\mathbf{u}\\| = \\sqrt{1^2 + 2^2} = \\sqrt{1 + 4} = \\sqrt{5} $$  \n",
    "   $$ \\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$  \n",
    "\n",
    "3. Compute $ \\cos \\theta $:  \n",
    "   $$ \\cos \\theta = \\frac{11}{\\sqrt{5} \\times 5} = \\frac{11}{5\\sqrt{5}} = \\frac{11\\sqrt{5}}{25} $$  \n",
    "\n",
    "4. Find $ \\theta $ using inverse cosine:  \n",
    "   $$ \\theta = \\cos^{-1} \\left( \\frac{11\\sqrt{5}}{25} \\right) $$  \n",
    "\n",
    "**Geometric Interpretation**    \n",
    "- If $ \\theta = 0^\\circ $, the vectors point in the same direction.  \n",
    "- If $ \\theta = 90^\\circ $, the vectors are perpendicular (orthogonal).  \n",
    "- If $ \\theta = 180^\\circ $, the vectors point in opposite directions.  \n",
    "\n",
    "**Machine Learning Application:**  \n",
    "- **Cosine Similarity:** Used in text analysis, recommendation systems, and clustering to measure the similarity between feature vectors.  \n",
    "- **Feature Engineering:** Helps decide whether two features are highly correlated.  \n",
    "\n",
    "---\n",
    "\n",
    "**Orthogonality (Perpendicular Vectors)**    \n",
    "\n",
    "Two vectors $ \\mathbf{u} $ and $ \\mathbf{v} $ are **orthogonal** if their dot product is zero:  \n",
    "\n",
    "$$ \\mathbf{u} \\cdot \\mathbf{v} = 0 $$  \n",
    "\n",
    "**Example:**  \n",
    "Consider $ \\mathbf{u} = (1, -2) $ and $ \\mathbf{v} = (2, 1) $:  \n",
    "$$ \\mathbf{u} \\cdot \\mathbf{v} = (1)(2) + (-2)(1) = 2 - 2 = 0 $$  \n",
    "Since the dot product is zero, the vectors are orthogonal.\n",
    "\n",
    "**Geometric Interpretation**    \n",
    "- Orthogonal vectors form a **right angle** (90°).  \n",
    "- In higher dimensions, orthogonal vectors define perpendicular directions in a vector space.  \n",
    "\n",
    "**Machine Learning Application:**  \n",
    "- **Principal Component Analysis (PCA):** Finds orthogonal directions of maximum variance to reduce dimensionality.  \n",
    "- **Regularization (Ridge and Lasso Regression):** Encourages independent (orthogonal) feature representations.  \n",
    "- **Neural Networks:** Orthogonal weight initialization helps stabilize training.  \n",
    "\n",
    "---\n",
    "\n",
    "**Orthonormal Vectors**    \n",
    "\n",
    "Vectors are **orthonormal** if they are both **orthogonal** and have unit length:  \n",
    "\n",
    "$$ \\|\\mathbf{u}\\| = 1, \\quad \\|\\mathbf{v}\\| = 1, \\quad \\text{and} \\quad \\mathbf{u} \\cdot \\mathbf{v} = 0 $$  \n",
    "\n",
    "**Example:**  \n",
    "The standard basis vectors in $ \\mathbb{R}^2 $ are:  \n",
    "$$ \\mathbf{e_1} = (1, 0), \\quad \\mathbf{e_2} = (0, 1) $$  \n",
    "They satisfy:  \n",
    "$$ \\mathbf{e_1} \\cdot \\mathbf{e_2} = (1)(0) + (0)(1) = 0 $$  \n",
    "$$ \\|\\mathbf{e_1}\\| = \\sqrt{1^2 + 0^2} = 1, \\quad \\|\\mathbf{e_2}\\| = \\sqrt{0^2 + 1^2} = 1 $$  \n",
    "Thus, they are **orthonormal**.\n",
    "\n",
    "**Machine Learning Application:**    \n",
    "- **Gram-Schmidt Process:** Converts a set of vectors into an orthonormal basis.  \n",
    "- **Eigenvectors in PCA:** The principal components form an orthonormal set.  \n",
    "\n",
    "---\n",
    "\n",
    "**Orthogonal Matrices and Their Use in Linear Transformations**  \n",
    "\n",
    "An **orthogonal matrix** is a square matrix $ Q \\in \\mathbb{R}^{n \\times n} $ whose columns (or rows) form an **orthonormal basis**. This means the columns are mutually **orthogonal** and have unit length.\n",
    "\n",
    "Mathematically, a matrix $ Q $ is **orthogonal** if:\n",
    "\n",
    "$$ Q^T Q = Q Q^T = I $$  \n",
    "\n",
    "where $ I $ is the identity matrix.\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of Orthogonal Matrices**    \n",
    "\n",
    "1. **Preserve Inner Products**  \n",
    "   If $ Q $ is an orthogonal matrix and $ \\mathbf{u}, \\mathbf{v} $ are vectors, then:  \n",
    "\n",
    "   $$ (Q\\mathbf{u}) \\cdot (Q\\mathbf{v}) = \\mathbf{u} \\cdot \\mathbf{v} $$  \n",
    "\n",
    "   This means that applying $ Q $ does not change angles or lengths.\n",
    "\n",
    "2. **Preserve Norms (Lengths)**  \n",
    "   For any vector $ \\mathbf{x} $:  \n",
    "\n",
    "   $$ \\| Q \\mathbf{x} \\| = \\| \\mathbf{x} \\| $$  \n",
    "\n",
    "3. **Determinant is ±1**  \n",
    "   $$ \\det(Q) = \\pm 1 $$  \n",
    "   - If $ \\det(Q) = 1 $, $ Q $ represents a **rotation**.  \n",
    "   - If $ \\det(Q) = -1 $, $ Q $ represents a **reflection**.  \n",
    "\n",
    "4. **Inverse is Transpose**  \n",
    "   $$ Q^{-1} = Q^T $$  \n",
    "   This simplifies matrix computations, especially in solving linear equations.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: 2D Rotation Matrix**    \n",
    "\n",
    "The standard **rotation matrix** in $ \\mathbb{R}^2 $:\n",
    "\n",
    "$$ Q = \n",
    "\\begin{bmatrix}\n",
    "\\cos \\theta & -\\sin \\theta \\\\\n",
    "\\sin \\theta & \\cos \\theta\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "- Rotates a vector by an angle $ \\theta $.  \n",
    "- Since $ Q^T Q = I $, it is **orthogonal**.\n",
    "\n",
    "**Applying to a vector**  \n",
    "Let $ \\mathbf{x} = (1,0) $ and rotate it by $ 90^\\circ $ ($ \\theta = \\frac{\\pi}{2} $):\n",
    "\n",
    "$$ Q = \n",
    "\\begin{bmatrix}\n",
    "\\cos \\frac{\\pi}{2} & -\\sin \\frac{\\pi}{2} \\\\\n",
    "\\sin \\frac{\\pi}{2} & \\cos \\frac{\\pi}{2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "Multiplying:\n",
    "\n",
    "$$ Q \\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "Thus, $ \\mathbf{x} = (1,0) $ is rotated to $ (0,1) $.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Reflection Matrix**    \n",
    "\n",
    "A reflection over the **x-axis** is given by:\n",
    "\n",
    "$$ Q =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "Applying to $ \\mathbf{x} = (3, 4) $:\n",
    "\n",
    "$$ Q \\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "-4\n",
    "\\end{bmatrix} $$  \n",
    "\n",
    "Thus, the point is reflected over the x-axis.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications of Orthogonal Matrices**    \n",
    "\n",
    "1. **Principal Component Analysis (PCA)**  \n",
    "   - Finds an **orthogonal basis** that maximizes variance in data.  \n",
    "   - Uses an **orthogonal transformation** to reduce dimensionality.  \n",
    "\n",
    "2. **Neural Networks (Weight Initialization)**  \n",
    "   - Orthogonal weight matrices help stabilize training in deep learning.  \n",
    "\n",
    "3. **QR Decomposition**  \n",
    "   - Factorizes a matrix into an **orthogonal matrix** ($ Q $) and an **upper triangular matrix** ($ R $).  \n",
    "   - Used in least squares regression and solving linear equations.  \n",
    "\n",
    "4. **Eigenvalue Decomposition**  \n",
    "   - **Symmetric matrices** have **orthogonal eigenvectors**, making them easier to diagonalize.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e908685e-f332-4ac4-afcb-633c2501c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean norm of v: 5.0\n",
      "The Manhattan norm of u: 3\n",
      "\n",
      "The Euclidean distance between u and v: 2.8284271247461903\n",
      "The Manhattan distance between u and v: 4\n",
      "\n",
      "The dot product of u and v: 11\n",
      "The weighted inner product of u and v: 32\n",
      "\n",
      "The wighted Euclidean norm of v is: 3.7416573867739413\n",
      "The wighted Euclidean distance of v is: 4.898979485566356\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([1, 2])\n",
    "v = np.array([3, 4])\n",
    "\n",
    "A = np.array([[2, 1],\n",
    "             [1, 2]])\n",
    "\n",
    "env = np.linalg.norm(v)\n",
    "mnu = np.sum(np.abs(u))\n",
    "\n",
    "ed = np.linalg.norm(u-v)\n",
    "md = np.sum(np.abs(u-v))\n",
    "\n",
    "dp = u @ v\n",
    "wp = u.T @ A @ v\n",
    "\n",
    "wnv = np.sqrt(u.T @ A @ u)\n",
    "wd = np.sqrt((u - v).T @ A @ (u - v))\n",
    "\n",
    "print(f\"The Euclidean norm of v: {env}\")\n",
    "print(f\"The Manhattan norm of u: {mnu}\")\n",
    "\n",
    "print(f\"\\nThe Euclidean distance between u and v: {ed}\")\n",
    "print(f\"The Manhattan distance between u and v: {md}\")\n",
    "\n",
    "print(f\"\\nThe dot product of u and v: {dp}\")\n",
    "print(f\"The weighted inner product of u and v: {wp}\")\n",
    "\n",
    "print(f\"\\nThe wighted Euclidean norm of v is: {wnv}\")\n",
    "print(f\"The wighted Euclidean distance of v is: {wd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e695154-ea55-4657-a51a-794bf0b9d450",
   "metadata": {},
   "source": [
    "# Orthonormal Basis\n",
    "\n",
    "An **orthonormal basis** of a vector space is a set of **orthonormal vectors** that spans the space.  \n",
    "\n",
    "A set of vectors $ \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\} $ in $ \\mathbb{R}^n $ is **orthonormal** if:  \n",
    "\n",
    "1. Each vector has **unit length** (norm = 1):  \n",
    "   $$ \\|\\mathbf{v}_i\\| = 1 \\quad \\forall i $$  \n",
    "2. Each pair of vectors is **orthogonal**:  \n",
    "   $$ \\mathbf{v}_i \\cdot \\mathbf{v}_j = 0 \\quad \\text{for } i \\neq j $$  \n",
    "\n",
    "If an orthonormal set of vectors spans a vector space, it forms an **orthonormal basis**.\n",
    "\n",
    "---\n",
    "**Example: Standard Basis in $ \\mathbb{R}^3 $**  \n",
    "The standard basis vectors in $ \\mathbb{R}^3 $ are:\n",
    "\n",
    "$$ \\mathbf{e}_1 = (1,0,0), \\quad \\mathbf{e}_2 = (0,1,0), \\quad \\mathbf{e}_3 = (0,0,1) $$  \n",
    "\n",
    "These satisfy:\n",
    "\n",
    "- **Unit length**:  \n",
    "  $$ \\|\\mathbf{e}_1\\| = \\|\\mathbf{e}_2\\| = \\|\\mathbf{e}_3\\| = 1 $$  \n",
    "- **Orthogonality**:  \n",
    "  $$ \\mathbf{e}_1 \\cdot \\mathbf{e}_2 = 0, \\quad \\mathbf{e}_1 \\cdot \\mathbf{e}_3 = 0, \\quad \\mathbf{e}_2 \\cdot \\mathbf{e}_3 = 0 $$  \n",
    "\n",
    "Thus, $ \\{\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3\\} $ is an **orthonormal basis** for $ \\mathbb{R}^3 $.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Orthonormal Basis from Any Basis (Gram-Schmidt Process)**  \n",
    "If given a set of **linearly independent vectors**, we can convert them into an **orthonormal basis** using the **Gram-Schmidt process**.\n",
    "\n",
    "Given two linearly independent vectors:\n",
    "\n",
    "$$ \\mathbf{v}_1 = (3,1), \\quad \\mathbf{v}_2 = (2,2) $$  \n",
    "\n",
    "Step 1: Normalize the first vector:  \n",
    "\n",
    "$$ \\mathbf{u}_1 = \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|} = \\frac{(3,1)}{\\sqrt{3^2 + 1^2}} = \\frac{(3,1)}{\\sqrt{10}} $$  \n",
    "\n",
    "Step 2: Remove the projection of $ \\mathbf{v}_2 $ onto $ \\mathbf{u}_1 $:  \n",
    "\n",
    "$$ \\mathbf{v}_2' = \\mathbf{v}_2 - \\text{proj}_{\\mathbf{u}_1} \\mathbf{v}_2 $$  \n",
    "$$ \\mathbf{v}_2' = (2,2) - \\frac{(2,2) \\cdot (3,1)}{\\|\\mathbf{u}_1\\|^2} \\mathbf{u}_1 $$  \n",
    "\n",
    "Step 3: Normalize $ \\mathbf{v}_2' $ to get $ \\mathbf{u}_2 $.\n",
    "\n",
    "This process ensures the new vectors are **orthogonal and unit-length**, forming an **orthonormal basis**.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications of Orthonormal Bases**  \n",
    "1. **Principal Component Analysis (PCA)**  \n",
    "   - Uses an orthonormal basis to represent data in lower dimensions.\n",
    "  \n",
    "2. **Fourier Transforms & Signal Processing**  \n",
    "   - Uses an **orthonormal basis** (Fourier basis) to represent signals efficiently.\n",
    "\n",
    "3. **Quantum Computing**  \n",
    "   - States are represented in an **orthonormal basis** of a Hilbert space.\n",
    "\n",
    "4. **Orthogonal Weight Initialization in Neural Networks**  \n",
    "   - Helps stabilize training and prevent gradient issues.\n",
    "\n",
    "Using an **orthonormal basis** simplifies calculations, improves numerical stability, and provides efficient ways to represent data in various domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bdd93-64c3-4831-9f73-543c16df5522",
   "metadata": {},
   "source": [
    "# Orthogonal Complement\n",
    " \n",
    "\n",
    "The **orthogonal complement** of a subspace $ W $ in a vector space $ V $ (with an inner product) is the set of all vectors in $ V $ that are **orthogonal** to every vector in $ W $.  \n",
    "\n",
    "Mathematically, the **orthogonal complement** of $ W $, denoted as $ W^\\perp $, is defined as:  \n",
    "\n",
    "$$ W^\\perp = \\{ \\mathbf{v} \\in V \\ | \\ \\mathbf{v} \\cdot \\mathbf{w} = 0, \\quad \\forall \\mathbf{w} \\in W \\} $$  \n",
    "\n",
    "This means that every vector in $ W^\\perp $ is **perpendicular** to every vector in $ W $.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 1: Orthogonal Complement in $ \\mathbb{R}^3 $**\n",
    "Consider the subspace $ W $ of $ \\mathbb{R}^3 $ spanned by the vector:\n",
    "\n",
    "$$ \\mathbf{w} = (1,2,3) $$  \n",
    "\n",
    "To find the **orthogonal complement** $ W^\\perp $, we look for all vectors $ \\mathbf{v} = (x,y,z) $ that satisfy:\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{v} = 0 $$  \n",
    "$$ (1,2,3) \\cdot (x,y,z) = 0 $$  \n",
    "$$ 1x + 2y + 3z = 0 $$  \n",
    "\n",
    "This defines a plane in $ \\mathbb{R}^3 $, meaning the **orthogonal complement** of a **1D subspace** is a **2D plane**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2: Orthogonal Complement in $ \\mathbb{R}^2 $**\n",
    "Let $ W $ be the **x-axis** in $ \\mathbb{R}^2 $, i.e.,  \n",
    "\n",
    "$$ W = \\text{span} \\{(1,0)\\} $$  \n",
    "\n",
    "A vector $ (x,y) $ is in the orthogonal complement $ W^\\perp $ if:\n",
    "\n",
    "$$ (1,0) \\cdot (x,y) = 0 $$  \n",
    "$$ 1x + 0y = 0 \\Rightarrow x = 0 $$  \n",
    "\n",
    "Thus, the **orthogonal complement** is the **y-axis**, i.e.,  \n",
    "\n",
    "$$ W^\\perp = \\text{span} \\{(0,1)\\} $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Properties of the Orthogonal Complement**\n",
    "1. **Dimensional Relationship**  \n",
    "   If $ W $ is a subspace of $ V $ of dimension $ k $, then:  \n",
    "   $$ \\dim(W) + \\dim(W^\\perp) = \\dim(V) $$  \n",
    "\n",
    "   For example, in $ \\mathbb{R}^3 $, if $ W $ is a **line (1D)**, then $ W^\\perp $ is a **plane (2D)**.\n",
    "\n",
    "2. **Double Complement Property**  \n",
    "   $$ (W^\\perp)^\\perp = W $$  \n",
    "\n",
    "   This means taking the orthogonal complement twice brings us back to the original subspace.\n",
    "\n",
    "3. **Orthogonal Decomposition**  \n",
    "   Every vector in $ V $ can be **uniquely** written as:  \n",
    "   $$ \\mathbf{v} = \\mathbf{w} + \\mathbf{w}^\\perp $$  \n",
    "   where $ \\mathbf{w} \\in W $ and $ \\mathbf{w}^\\perp \\in W^\\perp $.  \n",
    "   This is crucial in **least squares regression**.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications**\n",
    "1. **Least Squares Regression**  \n",
    "   - The residual vector (error) is **orthogonal** to the column space of the design matrix.\n",
    "   - This helps minimize errors in **linear regression**.\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**  \n",
    "   - PCA finds the **orthogonal complement** of the lower-dimensional subspace that best represents data variance.\n",
    "\n",
    "3. **Fourier Transforms**  \n",
    "   - Uses orthogonal complements to decompose signals into basis functions.\n",
    "\n",
    "The **orthogonal complement** provides insights into projections, decompositions, and dimensionality reduction in various applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d1edc-6717-4027-ae91-ef1292135be3",
   "metadata": {},
   "source": [
    "# Inner Product of Functions\n",
    "\n",
    "In functional analysis and machine learning, an **inner product of functions** extends the concept of the dot product to infinite-dimensional vector spaces. The inner product defines a measure of **similarity** between two functions, much like how the dot product measures similarity between vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**    \n",
    "The inner product of two functions $ f(x) $ and $ g(x) $ over an interval $ [a, b] $ with respect to a **weight function** $ w(x) $ is defined as:\n",
    "\n",
    "$$ \\langle f, g \\rangle = \\int_a^b f(x) g(x) w(x) \\, dx $$\n",
    "\n",
    "where:\n",
    "- $ f(x), g(x) $ are real or complex-valued functions,\n",
    "- $ w(x) $ is a **weight function** (default is $ w(x) = 1 $),\n",
    "- The integral computes a measure of similarity between $ f(x) $ and $ g(x) $.\n",
    "\n",
    "If $ \\langle f, g \\rangle = 0 $, the functions are **orthogonal** over $ [a, b] $.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 1: Standard Inner Product (No Weight Function)**  \n",
    "Consider the functions:\n",
    "\n",
    "$$ f(x) = x, \\quad g(x) = x^2 $$  \n",
    "\n",
    "on the interval $ [0,1] $. Their inner product is:\n",
    "\n",
    "$$ \\langle f, g \\rangle = \\int_0^1 x \\cdot x^2 \\, dx $$  \n",
    "\n",
    "$$ = \\int_0^1 x^3 \\, dx $$  \n",
    "\n",
    "$$ = \\frac{1}{4} $$  \n",
    "\n",
    "Since the inner product is **nonzero**, $ f(x) $ and $ g(x) $ are **not orthogonal**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2: Checking Orthogonality**    \n",
    "Let $ f(x) = \\sin(x) $ and $ g(x) = \\cos(x) $ over $ [0, \\pi] $. The inner product is:\n",
    "\n",
    "$$ \\langle f, g \\rangle = \\int_0^\\pi \\sin(x) \\cos(x) \\, dx $$  \n",
    "\n",
    "Using the identity:\n",
    "\n",
    "$$ \\sin(x) \\cos(x) = \\frac{1}{2} \\sin(2x) $$  \n",
    "\n",
    "We compute:\n",
    "\n",
    "$$ \\langle f, g \\rangle = \\int_0^\\pi \\frac{1}{2} \\sin(2x) \\, dx $$  \n",
    "\n",
    "Evaluating:\n",
    "\n",
    "$$ = \\frac{1}{2} \\left[ -\\frac{\\cos(2x)}{2} \\right]_0^\\pi $$  \n",
    "$$ = \\frac{1}{4} [ -\\cos(2\\pi) + \\cos(0) ] $$  \n",
    "$$ = \\frac{1}{4} [ -1 + 1 ] = 0 $$  \n",
    "\n",
    "Since the inner product is **zero**, $ \\sin(x) $ and $ \\cos(x) $ are **orthogonal**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3: Inner Product with a Weight Function**  \n",
    "Let:\n",
    "\n",
    "$$ f(x) = x, \\quad g(x) = 1 $$  \n",
    "\n",
    "over $ [0,1] $ with weight function $ w(x) = x $. The inner product is:\n",
    "\n",
    "$$ \\langle f, g \\rangle = \\int_0^1 x \\cdot 1 \\cdot x \\, dx $$  \n",
    "\n",
    "$$ = \\int_0^1 x^2 \\, dx $$  \n",
    "\n",
    "$$ = \\frac{1}{3} $$  \n",
    "\n",
    "This demonstrates how different **weight functions** impact the inner product.\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of the Inner Product**  \n",
    "1. **Linearity**:  \n",
    "   $$ \\langle af + bg, h \\rangle = a \\langle f, h \\rangle + b \\langle g, h \\rangle $$  \n",
    "\n",
    "2. **Symmetry**:  \n",
    "   $$ \\langle f, g \\rangle = \\langle g, f \\rangle $$  \n",
    "\n",
    "3. **Positive Definiteness**:  \n",
    "   $$ \\langle f, f \\rangle \\geq 0 $$  \n",
    "   and  \n",
    "   $$ \\langle f, f \\rangle = 0 \\iff f = 0 $$  \n",
    "\n",
    "4. **Orthogonality**:  \n",
    "   $$ \\langle f, g \\rangle = 0 \\Rightarrow f \\perp g $$  \n",
    "\n",
    "---\n",
    "\n",
    "**Applications in Machine Learning and Data Science**  \n",
    "- **Fourier Series & Signal Processing**:  \n",
    "  Functions are decomposed into orthogonal basis functions (e.g., sine and cosine waves).  \n",
    "\n",
    "- **Principal Component Analysis (PCA)**:  \n",
    "  PCA finds orthogonal directions (principal components) in data.  \n",
    "\n",
    "- **Kernel Methods in Machine Learning**:  \n",
    "  Inner products define similarity in **feature spaces** for algorithms like **SVMs** and **Gaussian Processes**.  \n",
    "\n",
    "- **Least Squares Regression**:  \n",
    "  The residual is **orthogonal** to the column space of the design matrix.  \n",
    "\n",
    "The **inner product of functions** is fundamental in functional analysis, physics, and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014ddba-3ed0-4cbb-bbd4-00066b4f171a",
   "metadata": {},
   "source": [
    "# Orthogonal Projections\n",
    "\n",
    "\n",
    "**General Concept of Projection**    \n",
    "A **projection** is the transformation of a vector onto another vector or subspace. It represents the closest approximation of a vector in one space onto another space. Projection is widely used in **computer graphics, physics, statistics, and machine learning**.\n",
    "\n",
    "If a vector **$ v $** is projected onto another vector **$ u $**, the resulting vector lies **along** $ u $. More generally, a vector can be projected onto a **subspace**, producing a shadow-like representation of that vector within the subspace.\n",
    "\n",
    "---\n",
    "\n",
    "**Orthogonal Projection**    \n",
    "The **orthogonal projection** of a vector **$ v $** onto a subspace is the closest point to $ v $ in that subspace. This means the difference between $ v $ and its projection is **perpendicular (orthogonal)** to the subspace.\n",
    "\n",
    "**Key Characteristics of Orthogonal Projection**  \n",
    "1. The difference between a vector and its projection is always **perpendicular** to the subspace.\n",
    "2. It minimizes the **distance** between the original vector and the subspace.\n",
    "3. The projection represents the **best approximation** of the vector within the given subspace.\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "If we project **$ v $** onto a **line** (1D subspace), the projection lies on that line. If we project onto a **plane** (2D subspace), the projection lies in that plane.\n",
    "\n",
    "- If **$ v $** is already in the subspace, its projection is **itself**.\n",
    "- If **$ v $** is not in the subspace, it gets **mapped** to the closest point in that subspace.\n",
    "\n",
    "**Applications of Projection in Machine Learning & Data Science**  \n",
    "- **Principal Component Analysis (PCA)**: Projects high-dimensional data onto a lower-dimensional subspace while preserving variance.\n",
    "- **Linear Regression**: The least squares solution finds the **projection** of observed data onto a linear model.\n",
    "- **Fourier Analysis**: Projects functions onto basis functions like sine and cosine waves.\n",
    "- **Computer Graphics**: Projection transformations are used for rendering 3D objects in 2D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b59099-7c20-4339-b76a-f2c32ed5e10f",
   "metadata": {},
   "source": [
    "## Projection onto One-Dimensional Subspaces (Lines)\n",
    "\n",
    " \n",
    "\n",
    "**Concept of Projection onto a Line**    \n",
    "A **one-dimensional subspace** is simply a **line** passing through the origin. If we have a vector **$ v $**, its projection onto a line spanned by a vector **$ u $** gives the closest approximation of $ v $ that lies along $ u $.\n",
    "\n",
    "Mathematically, the **projection of $ v $ onto $ u $** is given by:\n",
    "\n",
    "$$ \\text{Proj}_u v = \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u $$\n",
    "\n",
    "where:\n",
    "- **$ \\langle v, u \\rangle $** is the **dot product** of $ v $ and $ u $.\n",
    "- **$ \\langle u, u \\rangle $** is the **dot product** of $ u $ with itself (equivalent to $ ||u||^2 $, the squared norm of $ u $).\n",
    "- The scalar **$ \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} $** is the projection **coefficient**, determining how much of $ u $ is needed to best approximate $ v $.\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "The projection of $ v $ onto the line spanned by $ u $ is the **shadow** of $ v $ when a perpendicular is dropped from $ v $ onto the line. The vector **$ v - \\text{Proj}_u v $** is **orthogonal** to the line.\n",
    "\n",
    "This means:\n",
    "- **The projection lies on the line.**\n",
    "- **The error (difference) between $ v $ and its projection is perpendicular to the line.**\n",
    "\n",
    "---\n",
    "\n",
    "**Example Calculation**    \n",
    "Let’s say we have vectors:\n",
    "\n",
    "$$ v = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}, \\quad u = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$\n",
    "\n",
    "1. Compute the dot product **$ \\langle v, u \\rangle $**:\n",
    "\n",
    "   $$ \\langle v, u \\rangle = (3)(1) + (4)(2) = 3 + 8 = 11 $$\n",
    "\n",
    "2. Compute the squared norm of **$ u $**:\n",
    "\n",
    "   $$ ||u||^2 = \\langle u, u \\rangle = (1)^2 + (2)^2 = 1 + 4 = 5 $$\n",
    "\n",
    "3. Compute the projection coefficient:\n",
    "\n",
    "   $$ \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} = \\frac{11}{5} = 2.2 $$\n",
    "\n",
    "4. Multiply by **$ u $** to get the projection:\n",
    "\n",
    "   $$ \\text{Proj}_u v = 2.2 \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2.2 \\\\ 4.4 \\end{bmatrix} $$\n",
    "\n",
    "Thus, **$ v $** is projected onto the line as **$ [2.2, 4.4] $**.\n",
    "\n",
    "**Transformation Matrix for Projection**    \n",
    "\n",
    "**Projection Matrix Concept**    \n",
    "A **projection matrix** is a square matrix that, when applied to a vector, projects it onto a certain subspace. For **projection onto a line** (one-dimensional subspace), we can represent the projection operation as a matrix multiplication. The result of multiplying a vector by this matrix is the projection of the vector onto the subspace.\n",
    "\n",
    "For a vector **$ v $** and a line spanned by a unit vector **$ u $**, the projection matrix **$ P $** that projects any vector onto the line along **$ u $** is given by:\n",
    "\n",
    "$$ P = \\frac{u u^T}{u^T u} $$\n",
    "\n",
    "where:\n",
    "- **$ u u^T $** is the outer product of the vector **$ u $** with itself.\n",
    "- **$ u^T u $** is the squared norm of **$ u $**.\n",
    "\n",
    "Since **$ u $** is a unit vector in this case, **$ u^T u = 1 $**, and the projection matrix simplifies to:\n",
    "\n",
    "$$ P = u u^T $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example Calculation of Projection Matrix**  \n",
    "Consider a vector **$ u = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $**, and we want to compute the projection matrix onto the line spanned by **$ u $**.\n",
    "\n",
    "1. Compute the outer product **$ u u^T $**:\n",
    "\n",
    "   $$ u u^T = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} $$\n",
    "\n",
    "2. The projection matrix **$ P $** is:\n",
    "\n",
    "   $$ P = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "The transformation matrix **$ P $** represents a linear operation that **maps any vector to its projection onto the subspace spanned by $ u $**. When this matrix is applied to a vector, the result is the **closest point** on the line defined by **$ u $**.\n",
    "\n",
    "---\n",
    "\n",
    "**Projection Matrix for Higher Dimensions**  \n",
    "For projection onto a **higher-dimensional subspace** (e.g., a plane or hyperplane), the process is analogous. The matrix would be constructed using the **basis vectors** of the subspace. The general form for the projection matrix onto a subspace spanned by multiple vectors is:\n",
    "\n",
    "$$ P = A (A^T A)^{-1} A^T $$\n",
    "\n",
    "where:\n",
    "- **$ A $** is the matrix whose columns are the basis vectors of the subspace.\n",
    "- **$ A^T $** is the transpose of **$ A $**.\n",
    "- **$ (A^T A)^{-1} $** is the inverse of **$ A^T A $**, assuming it is invertible.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning & Data Science Applications**  \n",
    "- **Linear Regression**: The least squares solution finds the **projection** of observed data onto a model's feature space.\n",
    "- **PCA (Principal Component Analysis)**: Reduces dimensionality by projecting data onto **principal components** (lines).\n",
    "- **Fourier Analysis**: Projects functions onto sinusoidal basis functions.\n",
    "- **Error Minimization**: Projections are used to **minimize reconstruction error** in various optimization tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb01f9b-e9a7-433d-b5a2-16260ec312c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2, 4.4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([1, 2])\n",
    "v = np.array([3, 4])\n",
    "\n",
    "c = np.dot(u, v)/np.dot(u, u)\n",
    "proj_v_onto_u = c * u\n",
    "proj_v_onto_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1beec8c-a06c-467b-b2f8-5fb603fd117f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5e038-53d1-4aa0-85ea-5b8cd4406d91",
   "metadata": {},
   "source": [
    "## Projection onto General Subspaces\n",
    "\n",
    "\n",
    "**General Projection onto Subspaces**  \n",
    "In general, projecting a vector onto a subspace involves finding the closest vector in that subspace to the given vector. If you are projecting onto a subspace spanned by multiple vectors, the projection is determined by finding a linear combination of these vectors that minimizes the distance between the original vector and the subspace.\n",
    "\n",
    "Given a vector **$ v $** and a subspace **$ S $** spanned by multiple vectors **$ u_1, u_2, ..., u_k $**, the projection of **$ v $** onto **$ S $** is the vector in **$ S $** that is closest to **$ v $**. Mathematically, this can be achieved by solving the system of equations derived from the least squares problem.\n",
    "\n",
    "The formula for the projection of **$ v $** onto the subspace spanned by the vectors **$ u_1, u_2, ..., u_k $** is:\n",
    "\n",
    "$$ P = A (A^T A)^{-1} A^T $$\n",
    "\n",
    "where:\n",
    "- **$ A $** is the matrix whose columns are the basis vectors of the subspace **$ S $**.\n",
    "- **$ A^T $** is the transpose of **$ A $**.\n",
    "- **$ (A^T A)^{-1} $** is the inverse of **$ A^T A $**, assuming it is invertible.\n",
    "\n",
    "This formula computes the projection matrix that projects a vector onto the subspace **$ S $**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Projection onto a Subspace**  \n",
    "Consider a vector **$ v = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} $** and a subspace **$ S $** spanned by the vectors **$ u_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $** and **$ u_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $** (the standard basis of **$ \\mathbb{R}^2 $**). We want to project **$ v $** onto **$ S $**.\n",
    "\n",
    "Since **$ u_1 $** and **$ u_2 $** form an orthonormal basis for **$ \\mathbb{R}^2 $**, the projection matrix is simply the identity matrix:\n",
    "\n",
    "$$ P = I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "This projection is straightforward because **$ v $** is already in the subspace spanned by **$ u_1 $** and **$ u_2 $**.\n",
    "\n",
    "For a more complex subspace, such as a plane spanned by arbitrary vectors, we can use the projection formula outlined above.\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "The projection of **$ v $** onto **$ S $** is the **closest vector in the subspace** to **$ v $**. In higher-dimensional spaces, the projection is not always visually intuitive, but it can be thought of as the orthogonal \"shadow\" of the vector **$ v $** onto the subspace **$ S $**.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications**  \n",
    "- **Linear Regression**: The projection onto a subspace is used to find the **best-fit line** or **hyperplane** in linear regression. The projection matrix projects the data points onto the regression line or hyperplane that minimizes the error.\n",
    "- **PCA**: In Principal Component Analysis (PCA), the projection onto the principal components is the key operation. The data points are projected onto a lower-dimensional subspace defined by the eigenvectors of the covariance matrix.\n",
    "- **Dimensionality Reduction**: Techniques like PCA use projections to reduce the number of features in a dataset while retaining as much variance as possible.\n",
    "- **Signal Processing**: Projections are used in filtering and noise reduction, where the goal is to project the noisy signal onto a cleaner subspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678aeaf4-5d99-437c-8f9a-2db175ea161c",
   "metadata": {},
   "source": [
    "## Gram-Schmidt Orthogonalization\n",
    "\n",
    "**Introduction**  \n",
    "The **Gram-Schmidt process** is a method for converting a set of **linearly independent vectors** into an **orthonormal** set of vectors. This is useful in many applications, such as:\n",
    "- Constructing an **orthonormal basis** for a vector space.\n",
    "- Finding an **orthogonal projection** of a vector onto a subspace.\n",
    "- Decomposing a matrix in **QR factorization**, which is widely used in solving linear systems and optimization problems.\n",
    "\n",
    "If we have a set of **$ n $** linearly independent vectors **$ \\{v_1, v_2, \\dots, v_n\\} $**, the Gram-Schmidt process produces an orthonormal set **$ \\{q_1, q_2, \\dots, q_n\\} $** such that:\n",
    "1. The **$ q_i $** vectors are orthogonal (**$ q_i \\cdot q_j = 0 $** for **$ i \\neq j $**).\n",
    "2. Each **$ q_i $** has unit length (**$ ||q_i|| = 1 $**).\n",
    "\n",
    "---\n",
    "\n",
    "**Step-by-Step Gram-Schmidt Process**  \n",
    "Given linearly independent vectors **$ \\{v_1, v_2, ..., v_n\\} $**, we construct the orthonormal set **$ \\{q_1, q_2, ..., q_n\\} $** using the following procedure:\n",
    "\n",
    "1. **First Vector Normalization:**\n",
    "   - Define the first orthogonal vector **$ u_1 $** as:\n",
    "     $$ u_1 = v_1 $$\n",
    "   - Normalize it to obtain the first orthonormal vector:\n",
    "     $$ q_1 = \\frac{u_1}{||u_1||} $$\n",
    "\n",
    "2. **Second Vector Orthogonalization:**\n",
    "   - Remove the component of **$ v_2 $** along **$ q_1 $**:\n",
    "     $$ u_2 = v_2 - \\text{proj}_{q_1}(v_2) $$\n",
    "   - Compute the projection:\n",
    "     $$ \\text{proj}_{q_1}(v_2) = \\frac{q_1 \\cdot v_2}{q_1 \\cdot q_1} q_1 $$\n",
    "   - Normalize **$ u_2 $** to obtain:\n",
    "     $$ q_2 = \\frac{u_2}{||u_2||} $$\n",
    "\n",
    "3. **General Case for Any Vector** **$ v_k $**:**\n",
    "   - Orthogonalize **$ v_k $** against all previous **$ q_i $**:\n",
    "     $$ u_k = v_k - \\sum_{i=1}^{k-1} \\text{proj}_{q_i}(v_k) $$\n",
    "   - Normalize **$ u_k $** to obtain:\n",
    "     $$ q_k = \\frac{u_k}{||u_k||} $$\n",
    "\n",
    "This process is repeated for all **$ n $** vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Example**  \n",
    "Given two linearly independent vectors in **$ \\mathbb{R}^2 $**:\n",
    "\n",
    "$$ v_1 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}, \\quad v_2 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} $$\n",
    "\n",
    "1. **Compute the first orthonormal vector**:\n",
    "   - **$ u_1 = v_1 = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} $**\n",
    "   - Normalize:\n",
    "     $$ q_1 = \\frac{u_1}{||u_1||} = \\frac{1}{\\sqrt{10}} \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{3}{\\sqrt{10}} \\\\ \\frac{1}{\\sqrt{10}} \\end{bmatrix} $$\n",
    "\n",
    "2. **Compute the second orthonormal vector**:\n",
    "   - Projection of **$ v_2 $** onto **$ q_1 $**:\n",
    "     $$ \\text{proj}_{q_1}(v_2) = \\left( \\frac{q_1 \\cdot v_2}{q_1 \\cdot q_1} \\right) q_1 $$\n",
    "     $$ = \\left( \\frac{\\left(\\frac{3}{\\sqrt{10}}, \\frac{1}{\\sqrt{10}}\\right) \\cdot (2,2)}{1} \\right) q_1 $$\n",
    "     $$ = \\left( \\frac{6 + 2}{\\sqrt{10}} \\right) q_1 $$\n",
    "     $$ = \\left( \\frac{8}{\\sqrt{10}} \\right) q_1 $$\n",
    "     $$ = \\begin{bmatrix} \\frac{24}{10} \\\\ \\frac{8}{10} \\end{bmatrix} = \\begin{bmatrix} 2.4 \\\\ 0.8 \\end{bmatrix} $$\n",
    "\n",
    "   - Compute **$ u_2 $**:\n",
    "     $$ u_2 = v_2 - \\text{proj}_{q_1}(v_2) = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2.4 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} -0.4 \\\\ 1.2 \\end{bmatrix} $$\n",
    "\n",
    "   - Normalize **$ u_2 $** to get **$ q_2 $**:\n",
    "     $$ ||u_2|| = \\sqrt{(-0.4)^2 + (1.2)^2} = \\sqrt{0.16 + 1.44} = \\sqrt{1.6} $$\n",
    "     $$ q_2 = \\frac{u_2}{||u_2||} = \\frac{1}{\\sqrt{1.6}} \\begin{bmatrix} -0.4 \\\\ 1.2 \\end{bmatrix} = \\begin{bmatrix} -\\frac{0.4}{\\sqrt{1.6}} \\\\ \\frac{1.2}{\\sqrt{1.6}} \\end{bmatrix} $$\n",
    "\n",
    "Thus, the orthonormal basis is:\n",
    "\n",
    "$$ q_1 = \\begin{bmatrix} \\frac{3}{\\sqrt{10}} \\\\ \\frac{1}{\\sqrt{10}} \\end{bmatrix}, \\quad q_2 = \\begin{bmatrix} -\\frac{0.4}{\\sqrt{1.6}} \\\\ \\frac{1.2}{\\sqrt{1.6}} \\end{bmatrix} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "The Gram-Schmidt process **transforms a set of non-orthogonal basis vectors into an orthonormal set** by iteratively removing components in directions already accounted for. Geometrically:\n",
    "- The first vector remains unchanged.\n",
    "- The second vector is modified by removing its projection onto the first.\n",
    "- The third vector is modified by removing its projections onto the first two, and so on.\n",
    "\n",
    "This ensures that all vectors remain perpendicular.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications**  \n",
    "1. **QR Decomposition**: The Gram-Schmidt process is used to decompose a matrix into an **orthogonal matrix Q** and an **upper triangular matrix R**.\n",
    "2. **Principal Component Analysis (PCA)**: PCA involves finding **orthonormal basis vectors** (principal components) that maximize variance.\n",
    "3. **Orthogonal Projections**: The process is fundamental in least-squares regression, where projections are used to minimize residual errors.\n",
    "4. **Stability in Numerical Computation**: Many algorithms in optimization and statistics require an orthonormal basis to avoid numerical instability.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "- **Gram-Schmidt orthogonalizes a set of vectors**, turning them into an **orthonormal basis**.\n",
    "- The process removes components of one vector in the direction of others to maintain orthogonality.\n",
    "- It is widely used in **QR decomposition, PCA, and numerical optimization**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6890b5-b507-465b-95ed-b2ba6617e094",
   "metadata": {},
   "source": [
    "## Projection onto Affine Subspaces\n",
    "\n",
    "\n",
    "**Introduction**  \n",
    "An **affine subspace** in **$ \\mathbb{R}^n $** is a translation of a linear subspace. Unlike a linear subspace, which passes through the origin, an affine subspace may be **shifted** away from the origin.\n",
    "\n",
    "If we want to project a vector **$ x $** onto an affine subspace, we need to:\n",
    "1. **Identify a point on the subspace** (say **$ p $**).\n",
    "2. **Find the closest point to $ x $** that belongs to the affine subspace.\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Definition**  \n",
    "Let:\n",
    "- **$ S $** be an affine subspace of **$ \\mathbb{R}^n $**, defined as:\n",
    "  $$ S = \\{ p + y \\mid y \\in V \\} $$\n",
    "  where **$ p $** is a fixed point in the subspace and **$ V $** is a linear subspace.\n",
    "- **$ x \\in \\mathbb{R}^n $** be a vector we want to project onto **$ S $**.\n",
    "\n",
    "The **projection of $ x $ onto $ S $**, denoted as **$ \\text{proj}_S(x) $**, is given by:\n",
    "\n",
    "$$ \\text{proj}_S(x) = p + \\text{proj}_V(x - p) $$\n",
    "\n",
    "where **$ \\text{proj}_V(x - p) $** is the **orthogonal projection** of **$ x - p $** onto the subspace **$ V $**.\n",
    "\n",
    "---\n",
    "\n",
    "**Computing the Projection**  \n",
    "To compute **$ \\text{proj}_S(x) $**, follow these steps:\n",
    "\n",
    "1. **Find an orthonormal basis for $ V $** (using Gram-Schmidt if needed).\n",
    "2. **Compute the projection of $ x - p $** onto **$ V $** using the standard projection formula:\n",
    "   $$ \\text{proj}_V(x - p) = VV^T (x - p) $$\n",
    "   where **$ V $** is a matrix whose columns are the basis vectors of **$ V $**.\n",
    "3. **Add back $ p $** to get the projection onto **$ S $**:\n",
    "   $$ \\text{proj}_S(x) = p + VV^T (x - p) $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example**  \n",
    "Consider **$ \\mathbb{R}^3 $** with an affine subspace **$ S $** defined by:\n",
    "\n",
    "- **A point on the subspace**: $ p = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} $\n",
    "- **A basis for the subspace's direction**:\n",
    "  $$ V = \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\} $$\n",
    "\n",
    "We want to **project** the point:\n",
    "\n",
    "$$ x = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} $$\n",
    "\n",
    "onto **$ S $**.\n",
    "\n",
    "1. **Construct matrix $ V $** from basis vectors:\n",
    "   $$ V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\n",
    "\n",
    "2. **Compute $ VV^T $**:\n",
    "   $$ VV^T = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix} $$\n",
    "\n",
    "3. **Compute $ \\text{proj}_V(x - p) $**:\n",
    "   $$ x - p = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} $$\n",
    "\n",
    "   $$ \\text{proj}_V(x - p) = VV^T (x - p) $$\n",
    "\n",
    "   $$ = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix} $$\n",
    "\n",
    "   $$ = \\begin{bmatrix} (1)(3) + (0)(3) + (1)(3) \\\\ (0)(3) + (1)(3) + (1)(3) \\\\ (1)(3) + (1)(3) + (2)(3) \\end{bmatrix} $$\n",
    "\n",
    "   $$ = \\begin{bmatrix} 6 \\\\ 6 \\\\ 12 \\end{bmatrix} $$\n",
    "\n",
    "4. **Compute $ \\text{proj}_S(x) $**:\n",
    "   $$ \\text{proj}_S(x) = p + \\text{proj}_V(x - p) $$\n",
    "\n",
    "   $$ = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\ 6 \\\\ 12 \\end{bmatrix} $$\n",
    "\n",
    "   $$ = \\begin{bmatrix} 7 \\\\ 8 \\\\ 15 \\end{bmatrix} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "- The vector **$ x $** is **projected** onto the affine subspace **$ S $**, meaning we find the **closest point** to **$ x $** in **$ S $**.\n",
    "- The process involves:\n",
    "  - **Shifting** the problem so that the subspace passes through the origin.\n",
    "  - **Projecting onto the linear subspace**.\n",
    "  - **Shifting back** to the affine subspace.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications in Machine Learning**  \n",
    "1. **Least Squares Regression**: The **solution to a least-squares problem is a projection** onto an affine subspace.\n",
    "2. **Dimensionality Reduction**: PCA projects data onto a lower-dimensional **affine subspace** that captures the most variance.\n",
    "3. **Optimization and Control**: Many optimization problems require projecting onto affine constraints.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "- **Affine subspaces are linear subspaces shifted away from the origin**.\n",
    "- **Projection onto an affine subspace** is done by:\n",
    "  1. **Shifting** to a linear subspace.\n",
    "  2. **Projecting** onto that subspace.\n",
    "  3. **Shifting back**.\n",
    "- The projection is computed using:\n",
    "  $$ \\text{proj}_S(x) = p + VV^T (x - p) $$\n",
    "- This concept is essential in **regression, PCA, and optimization**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd0b32-2320-4c4d-b5a7-4d069d803622",
   "metadata": {},
   "source": [
    "# Rotations\n",
    "\n",
    "## Rotation in 2D Space \n",
    "\n",
    "**Introduction**  \n",
    "A rotation in **$\\mathbb{R}^2$** is a transformation that rotates a vector **counterclockwise** by an angle **$\\theta$** around the origin. Rotations are **linear transformations** and can be represented using a **rotation matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation Matrix in $\\mathbb{R}^2$**  \n",
    "The standard **rotation matrix** for a **counterclockwise** rotation by an angle **$\\theta$** in **$\\mathbb{R}^2$** is:\n",
    "\n",
    "$$\n",
    "R(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Given a vector **$ v = \\begin{bmatrix} x \\\\ y \\end{bmatrix} $**, its rotated version **$ v' $** is given by:\n",
    "\n",
    "$$\n",
    "v' = R(\\theta) v\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "x' = x\\cos\\theta - y\\sin\\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "y' = x\\sin\\theta + y\\cos\\theta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example Calculation**  \n",
    "Let’s rotate the point **$(2, 3)$** by **$45^\\circ$** (**$\\theta = \\frac{\\pi}{4}$**):\n",
    "\n",
    "1. Compute **$\\cos 45^\\circ = \\frac{\\sqrt{2}}{2}$** and **$\\sin 45^\\circ = \\frac{\\sqrt{2}}{2}$**.\n",
    "2. Apply the rotation matrix:\n",
    "\n",
    "$$\n",
    "R(45^\\circ) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(2)(\\frac{\\sqrt{2}}{2}) + (-3)(\\frac{\\sqrt{2}}{2}) \\\\\n",
    "(2)(\\frac{\\sqrt{2}}{2}) + (3)(\\frac{\\sqrt{2}}{2})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{2\\sqrt{2}}{2} - \\frac{3\\sqrt{2}}{2} \\\\\n",
    "\\frac{2\\sqrt{2}}{2} + \\frac{3\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-\\frac{\\sqrt{2}}{2} \\\\\n",
    "\\frac{5\\sqrt{2}}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, the rotated point is approximately **(-0.71, 3.54)**.\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "- Rotation **preserves the length** of vectors (i.e., it is an **orthogonal transformation**).\n",
    "- Rotation **does not change the origin**.\n",
    "- The axes remain **perpendicular** to each other.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications**  \n",
    "1. **Data Augmentation**: Rotations are commonly used in image processing to artificially expand datasets.\n",
    "2. **Computer Vision**: Used in **object tracking** and **pose estimation**.\n",
    "3. **Feature Engineering**: PCA (Principal Component Analysis) often involves rotation of axes to align data along principal components.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "- Rotation in **$\\mathbb{R}^2$** is given by:\n",
    "  \n",
    "  $$\n",
    "  R(\\theta) =\n",
    "  \\begin{bmatrix}\n",
    "  \\cos\\theta & -\\sin\\theta \\\\\n",
    "  \\sin\\theta & \\cos\\theta\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- Rotating a vector **$ (x, y) $** by **$ \\theta $** results in:\n",
    "\n",
    "  $$\n",
    "  x' = x\\cos\\theta - y\\sin\\theta\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  y' = x\\sin\\theta + y\\cos\\theta\n",
    "  $$\n",
    "\n",
    "- **Applications** include **computer vision, image processing, and data transformations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6475960-b601-47ee-915e-da676fdeb373",
   "metadata": {},
   "source": [
    "## Rotation in 3D Space\n",
    "\n",
    "**Introduction**  \n",
    "Rotation in **$\\mathbb{R}^3$** extends the concept of **2D rotation** by introducing rotations about the **x, y, and z axes**. Unlike **$\\mathbb{R}^2$**, where there is only one plane of rotation, **3D space** allows for rotations about multiple axes.\n",
    "\n",
    "A **rotation in 3D** is still a **linear transformation**, meaning it can be represented using **rotation matrices**.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation Matrices in $\\mathbb{R}^3$**  \n",
    "In **3D space**, we define three fundamental rotation matrices:\n",
    "1. **Rotation about the x-axis** by an angle **$\\theta$**:\n",
    "   \n",
    "   $$\n",
    "   R_x(\\theta) =\n",
    "   \\begin{bmatrix}\n",
    "   1 & 0 & 0 \\\\\n",
    "   0 & \\cos\\theta & -\\sin\\theta \\\\\n",
    "   0 & \\sin\\theta & \\cos\\theta\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Rotation about the y-axis** by an angle **$\\theta$**:\n",
    "\n",
    "   $$\n",
    "   R_y(\\theta) =\n",
    "   \\begin{bmatrix}\n",
    "   \\cos\\theta & 0 & \\sin\\theta \\\\\n",
    "   0 & 1 & 0 \\\\\n",
    "   -\\sin\\theta & 0 & \\cos\\theta\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Rotation about the z-axis** by an angle **$\\theta$**:\n",
    "\n",
    "   $$\n",
    "   R_z(\\theta) =\n",
    "   \\begin{bmatrix}\n",
    "   \\cos\\theta & -\\sin\\theta & 0 \\\\\n",
    "   \\sin\\theta & \\cos\\theta & 0 \\\\\n",
    "   0 & 0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "Each of these matrices **rotates** a **point** or **vector** around a specific axis while keeping that axis fixed.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Rotating a Point Around the z-Axis**  \n",
    "Suppose we want to rotate the point **$(1, 2, 3)$** by **$90^\\circ$** (or **$\\frac{\\pi}{2}$ radians**) around the **z-axis**.\n",
    "\n",
    "1. Compute **$\\cos 90^\\circ = 0$** and **$\\sin 90^\\circ = 1$**.\n",
    "2. Use the **z-axis rotation matrix**:\n",
    "\n",
    "   $$\n",
    "   R_z(90^\\circ) =\n",
    "   \\begin{bmatrix}\n",
    "   0 & -1 & 0 \\\\\n",
    "   1 & 0 & 0 \\\\\n",
    "   0 & 0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. Apply the transformation:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix} x' \\\\ y' \\\\ z' \\end{bmatrix} =\n",
    "   R_z(90^\\circ) \\cdot\n",
    "   \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   =\n",
    "   \\begin{bmatrix}\n",
    "   0 & -1 & 0 \\\\\n",
    "   1 & 0 & 0 \\\\\n",
    "   0 & 0 & 1\n",
    "   \\end{bmatrix}\n",
    "   \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   $$=\n",
    "   \\begin{bmatrix}\n",
    "   (0)(1) + (-1)(2) + (0)(3) \\\\\n",
    "   (1)(1) + (0)(2) + (0)(3) \\\\\n",
    "   (0)(1) + (0)(2) + (1)(3)\n",
    "   \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix} -2 \\\\ 1 \\\\ 3 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "Thus, the rotated point is **$(-2, 1, 3)$**.\n",
    "\n",
    "---\n",
    "\n",
    "**Geometric Interpretation**  \n",
    "- Rotations in **3D space** preserve the **length** of vectors (**orthogonal transformation**).\n",
    "- They **preserve angles** between vectors.\n",
    "- They **do not change the origin**.\n",
    "- Composing rotations about different axes in sequence **does not commute** in general.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation About an Arbitrary Axis**  \n",
    "In **3D**, we can also **rotate about an arbitrary axis** defined by a unit vector **$\\hat{u} = (u_x, u_y, u_z)$** using Rodrigues’ rotation formula:\n",
    "\n",
    "   $$\n",
    "   R(\\theta) = I + (\\sin\\theta)K + (1 - \\cos\\theta)K^2\n",
    "   $$\n",
    "\n",
    "where **$K$** is the **skew-symmetric matrix** of **$\\hat{u}$**:\n",
    "\n",
    "   $$\n",
    "   K =\n",
    "   \\begin{bmatrix}\n",
    "   0 & -u_z & u_y \\\\\n",
    "   u_z & 0 & -u_x \\\\\n",
    "   -u_y & u_x & 0\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "This allows for **free rotation around any vector** in space.\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Learning Applications**  \n",
    "1. **3D Data Augmentation**: Rotations are used in **3D computer vision** (e.g., **point clouds, LiDAR scans**).\n",
    "2. **Robotics & Kinematics**: Used for **robot arm movement** and **drone orientation**.\n",
    "3. **Graphics & Simulations**: Game engines use 3D rotation matrices for **character movement and animations**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "- **Rotations in $\\mathbb{R}^3$** are represented by **rotation matrices** about the **x, y, and z** axes.\n",
    "- The **rotation matrix** depends on the axis and **angle $\\theta$**.\n",
    "- Rotations **preserve distances and angles**.\n",
    "- **More complex rotations** can be handled using **Rodrigues' formula** or **quaternions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca750e0-5fe1-4d35-b14e-025b8ab937f1",
   "metadata": {},
   "source": [
    "## Rotation in nD space\n",
    "\n",
    "**Introduction**  \n",
    "Rotation in **$\\mathbb{R}^n$** generalizes the concept of **2D and 3D rotations** to **n-dimensional space**. A **rotation** is a **linear transformation** that preserves the **lengths of vectors** and the **angles** between them. \n",
    "\n",
    "In **$\\mathbb{R}^n$**, a rotation can be defined using an **orthogonal matrix** $R$ such that:\n",
    "\n",
    "$$\n",
    "R^T R = I\n",
    "$$\n",
    "\n",
    "where $I$ is the **identity matrix**, ensuring that the transformation **preserves vector norms**.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation Matrices in $\\mathbb{R}^n$**  \n",
    "A general **rotation matrix** in **n-dimensional space** is an **orthogonal matrix** with **determinant $1$**:\n",
    "\n",
    "$$\n",
    "R \\in \\mathbb{R}^{n \\times n}, \\quad R^T R = I, \\quad \\det(R) = 1\n",
    "$$\n",
    "\n",
    "For any vector **$x \\in \\mathbb{R}^n$**, its rotated version is given by:\n",
    "\n",
    "$$\n",
    "x' = R x\n",
    "$$\n",
    "\n",
    "where **$R$** is an **$n \\times n$ rotation matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation in Higher Dimensions**  \n",
    "In **$\\mathbb{R}^n$**, a rotation occurs within a **plane spanned by two basis vectors**. Any rotation can be defined by selecting two coordinate axes and rotating within that **2D subspace** while leaving the remaining dimensions unchanged.\n",
    "\n",
    "A **Givens rotation matrix** is a simple rotation within a **2D plane inside $\\mathbb{R}^n$**:\n",
    "\n",
    "$$\n",
    "G(i, j, \\theta) = I + (R_{ij} - I)\n",
    "$$\n",
    "\n",
    "where $R_{ij}$ is a **2D rotation matrix** applied to dimensions $i$ and $j$, while the other dimensions remain unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Rotation in $\\mathbb{R}^4$**  \n",
    "Consider a **4D vector**:\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To rotate it in the **$x_1-x_2$ plane** by an angle **$\\theta$**, we use:\n",
    "\n",
    "$$\n",
    "R_{12}(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta & 0 & 0 \\\\\n",
    "\\sin\\theta & \\cos\\theta & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying this to **$x$** results in a rotation **only within the $x_1-x_2$ plane**, leaving $x_3$ and $x_4$ unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "**General Rotation in $\\mathbb{R}^n$**  \n",
    "A general rotation matrix can be constructed using **$n(n-1)/2$ elementary 2D rotations**, each rotating along a **plane spanned by two dimensions**. \n",
    "\n",
    "A **higher-dimensional rotation** can be represented as a **composition** of these **elementary rotations**.\n",
    "\n",
    "For any two **unit vectors** $u, v \\in \\mathbb{R}^n$, the **rotation matrix** that rotates **$u$ onto $v$** is:\n",
    "\n",
    "$$\n",
    "R = I + (v - u) u^T + (u - v) v^T\n",
    "$$\n",
    "\n",
    "where the transformation aligns the vector **$u$** with **$v$**.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications in Machine Learning**  \n",
    "1. **Data Augmentation**: Rotations in **higher dimensions** are used for **image transformations, 3D object recognition**, and **augmented data generation**.\n",
    "2. **Principal Component Analysis (PCA)**: PCA uses **rotation matrices** to **align** the data along its **principal axes**.\n",
    "3. **Neural Networks & Optimization**: Weight transformations often involve **rotations in high-dimensional space**.\n",
    "4. **Quantum Computing**: Rotation matrices play a role in **unitary transformations** in quantum mechanics.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "- **Rotations in $\\mathbb{R}^n$** are **orthogonal transformations** preserving **distances and angles**.\n",
    "- A **rotation** occurs within a **2D subspace** of **$\\mathbb{R}^n$**.\n",
    "- **Givens rotations** allow for **efficient** computation of rotations in high-dimensional spaces.\n",
    "- Rotations are fundamental in **machine learning**, **dimensionality reduction**, and **computer graphics**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b361d-32dc-46c6-8e82-d4485f3a6d79",
   "metadata": {},
   "source": [
    "## Properties of Rotations\n",
    "\n",
    "\n",
    "A **rotation** in **$\\mathbb{R}^n$** is a **linear transformation** that **preserves distances** and **angles** between vectors.The transformation is represented by an **orthogonal matrix** $R$ such that:\n",
    "\n",
    "$$\n",
    "R^T R = I\n",
    "$$\n",
    "\n",
    "where **$I$** is the **identity matrix**. Additionally, for a proper rotation:\n",
    "\n",
    "$$\n",
    "\\det(R) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Properties of Rotation Matrices**    \n",
    "*(a) Rotations Preserve Length (Norm Preservation)*  \n",
    "For any vector $x \\in \\mathbb{R}^n$, its length (Euclidean norm) remains unchanged after rotation:\n",
    "\n",
    "$$\n",
    "\\| R x \\| = \\| x \\|\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "Using the definition of rotation matrices:\n",
    "\n",
    "$$\n",
    "\\| R x \\|^2 = (R x)^T (R x) = x^T R^T R x = x^T I x = \\| x \\|^2\n",
    "$$\n",
    "\n",
    "Thus, rotations do not change vector magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "*(b) Rotations Preserve Angles*  \n",
    "For any two vectors $x, y \\in \\mathbb{R}^n$, the **angle between them** remains unchanged after applying a rotation matrix:\n",
    "\n",
    "$$\n",
    "\\cos\\theta = \\frac{x^T y}{\\|x\\| \\|y\\|}\n",
    "$$\n",
    "\n",
    "Since rotation preserves the dot product:\n",
    "\n",
    "$$\n",
    "(Rx)^T (Ry) = x^T R^T R y = x^T y\n",
    "$$\n",
    "\n",
    "it follows that the angle $\\theta$ between vectors is unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "*(c) Composition of Rotations is a Rotation*  \n",
    "If **$R_1$** and **$R_2$** are two rotation matrices, their product **$R_3 = R_1 R_2$** is also a rotation matrix:\n",
    "\n",
    "$$\n",
    "(R_1 R_2)^T (R_1 R_2) = R_2^T R_1^T R_1 R_2 = R_2^T I R_2 = R_2^T R_2 = I\n",
    "$$\n",
    "\n",
    "Thus, the set of all rotation matrices forms a **group** under matrix multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "*(d) Inverse of a Rotation is Its Transpose*  \n",
    "For any rotation matrix $R$, its **inverse** is equal to its **transpose**:\n",
    "\n",
    "$$\n",
    "R^{-1} = R^T\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "From the definition:\n",
    "\n",
    "$$\n",
    "R^T R = I\n",
    "$$\n",
    "\n",
    "Multiplying both sides by **$R^{-1}$**, we get:\n",
    "\n",
    "$$\n",
    "R^{-1} = R^T\n",
    "$$\n",
    "\n",
    "Thus, rotating by **$R$** and then by **$R^T$** returns the vector to its original position.\n",
    "\n",
    "---\n",
    "\n",
    "*(e) Determinant of a Rotation Matrix*  \n",
    "A **proper rotation** matrix has a determinant of **1**:\n",
    "\n",
    "$$\n",
    "\\det(R) = 1\n",
    "$$\n",
    "\n",
    "This ensures that the transformation **preserves orientation** (i.e., no reflection occurs).\n",
    "\n",
    "---\n",
    "\n",
    "*(f) Rotations Do Not Change Volume*  \n",
    "For any **region** in $\\mathbb{R}^n$, the volume remains unchanged after rotation:\n",
    "\n",
    "$$\n",
    "\\text{Volume}(R S) = \\text{Volume}(S)\n",
    "$$\n",
    "\n",
    "where $S$ is a geometric object, such as a parallelepiped or hypersphere.\n",
    "\n",
    "**Proof:**\n",
    "Since **rotation matrices have determinant 1**, they preserve **volume**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Special Properties in $\\mathbb{R}^2$ and $\\mathbb{R}^3$**  \n",
    "*(a) 2D Rotation Matrix*  \n",
    "A **rotation matrix** in **$\\mathbb{R}^2$** for an angle **$\\theta$** is:\n",
    "\n",
    "$$\n",
    "R(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*(b) 3D Rotation Matrices*  \n",
    "In **$\\mathbb{R}^3$**, rotation matrices are defined about coordinate axes:\n",
    "\n",
    "- **Rotation about the $x$-axis:**\n",
    "  $$\n",
    "  R_x(\\theta) =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & 0 \\\\\n",
    "  0 & \\cos\\theta & -\\sin\\theta \\\\\n",
    "  0 & \\sin\\theta & \\cos\\theta\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Rotation about the $y$-axis:**\n",
    "  $$\n",
    "  R_y(\\theta) =\n",
    "  \\begin{bmatrix}\n",
    "  \\cos\\theta & 0 & \\sin\\theta \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  -\\sin\\theta & 0 & \\cos\\theta\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Rotation about the $z$-axis:**\n",
    "  $$\n",
    "  R_z(\\theta) =\n",
    "  \\begin{bmatrix}\n",
    "  \\cos\\theta & -\\sin\\theta & 0 \\\\\n",
    "  \\sin\\theta & \\cos\\theta & 0 \\\\\n",
    "  0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**4. Applications in Machine Learning & Data Science**  \n",
    "**(a) Principal Component Analysis (PCA)**  \n",
    "- PCA uses **rotation matrices** to **align** data with its **principal components**.\n",
    "- This allows for **dimensionality reduction** and better visualization.\n",
    "\n",
    "**(b) Data Augmentation in Computer Vision**  \n",
    "- **Rotation transformations** are used to **augment datasets** by generating rotated versions of images.\n",
    "- Helps train **neural networks** to be **rotation-invariant**.\n",
    "\n",
    "**(c) Robotics & Computer Graphics**  \n",
    "- **3D object transformations** use rotation matrices for **camera movements** and **robot arm movements**.\n",
    "\n",
    "**(d) Optimization in High-Dimensional Spaces**  \n",
    "- In **gradient-based optimizations**, rotation matrices are used to **reorient** coordinate systems.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Summary**  \n",
    "- Rotations **preserve length and angles**.\n",
    "- They are represented by **orthogonal matrices** with **determinant 1**.\n",
    "- The **inverse** of a rotation is **its transpose**.\n",
    "- They **preserve volume** and do not introduce distortions.\n",
    "- **Essential** in **machine learning, data science, and engineering**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49438b49-9043-4faf-b99e-64ac3553fad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data-analysis)",
   "language": "python",
   "name": "data-anaysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
